# Splunk Architecture

## 1. Data Sources

## 2. Parsing Queue

## 3. Parsing Pipeline
**Description:** Where data is converted into events so all the normalization, regex transformation, and others mentioned bloew is done on the parsing pipeline
* source, event typing
* charactr set normalization
* line termination identificaiton
* timestamp identificaiton / normalization, event boundary identification
* regex transfomrs, partse-time field creation

## Indexes Overview
**Description:** index is a repository of Splunk data.
* Splunk transforms incoming data into events, which it sotres in the indexes.


### 4. Index Queue

### 5. Indexing Pipeline Indexes
**Description:**
* segmentation
* index building
* When Splunk indexes data, it creates number of files that fall into two main categories:
  1. The raw data in compressed form (rawdata)
  2. indexes that points to raw data (tsidx files), plus some meta-data files


#### Createomg Index in Splunk ES
* Search 'Settings' > 'Indexes' > 'New Index'
  
  ![image](https://github.com/user-attachments/assets/d3c59c9d-c49f-4d43-992e-8c8b9b5c433a)

* index name = kplabs > press 'save'
  
  ![image](https://github.com/user-attachments/assets/e61f91c5-a759-4c99-8b84-d4d7dafd3ca8)

* index 'kplabs' created

  ![image](https://github.com/user-attachments/assets/ecbc9672-6778-4177-88ff-67bf4ab6650f)

* In Command Line type, **/opt/splunk/var/lib/splunk** and that is the directory where indexes are stores (including kplabs just created).

  ![image](https://github.com/user-attachments/assets/087a72cd-377a-4f08-bc94-50dceb9f9d31)

* Create a sample data in Notepad

  ![image](https://github.com/user-attachments/assets/70af545c-64d4-4458-89c3-ae05eab20be0)

#### Adding Data
* Select 'Settings' > 'Add Data' > Upload

  ![image](https://github.com/user-attachments/assets/372db915-7468-44db-9120-559d95fbb262)

* Select file just created in Notepad > click 'Next'

  ![image](https://github.com/user-attachments/assets/76a9e8cd-d8e2-4c1b-81e4-c67ace5998e7)

* Event break as 'every line' > Next

  ![image](https://github.com/user-attachments/assets/c095df29-ae5a-48b0-9c5b-00d0328c1d20)

* click 'Save'

  ![image](https://github.com/user-attachments/assets/f9fd5c8f-59c1-4a3d-a5c3-1f78e5b1dd86)

* add this data to 'kplabs' index > submit

  ![image](https://github.com/user-attachments/assets/f74c2f39-c92f-43b0-af7d-5c20c3334f19)

* Within 'Search & Reporting' app, events appear

  ![image](https://github.com/user-attachments/assets/df6877e9-f003-4c43-8327-87dc0f3d38be)

* In the Command Line, type **/opt/splunk/var/lib/splunk/kplabs/db/hot_v1_0 ls -l** > you will see **rawdata** directory. Type **cat 0** to view raw data that you created

  <img width="932" alt="image" src="https://github.com/user-attachments/assets/d3054862-0478-413b-9de4-f0affd72f056" />

  <img width="932" alt="image" src="https://github.com/user-attachments/assets/dd4ce8fa-33f2-4e8d-b86f-455ab20c35d3" />

## Splunk Bucket Lifecycle
**Definition:** Splunk stores all its data in directories on server called buckets. Read + Write.
* A bucket moves through several states as it ages
  1. Hot: Most expensive and fast disk. All of the new data is written here and most recent data is kept here. Read Only. CANNOT backup 'Hot' bucket.
  2. Warm: Less expensive disk. Data rolled form hot. Data is not actively writte to warm buckets. Read Only.
  3. Cold: Less expensive disk.Data rolled from warm. Rarely searched data as it has aged / archived. Read Only.
  4. Frozen: Data rolled from cold. The data is deleted, but can be archived. Data is deleted by default, but you can configure it to archive it instead. Read Only.
  5. Thawed: If data in frozen bucket is archived, it can be indexed again by thawing. Read Only.

* Hot and Warm bucket are in the same disk.

### Splunk ES Practice

* Create an index called "bucketlifecycle"
  
  ![image](https://github.com/user-attachments/assets/b7a598d2-7754-4dfc-97ae-a77e230f40a4)
  ![image](https://github.com/user-attachments/assets/4b29d468-4a22-4d24-b288-13a5d9ed1bfa)

* Within Command Line, type **/opt/splunk/var/lib/splunk ls -l** and find the index recently created

  ![image](https://github.com/user-attachments/assets/8c247980-e8db-4f13-8595-65c85224d096)

* Within 'bucketlifecycle' index, you'll find colddb (Cold Bucket) and db (Hot/Warm buckets)

  <img width="510" alt="image" src="https://github.com/user-attachments/assets/34545f39-5aa8-4112-8364-5efa252d6eeb" />

* Add Data by clicking on 'Settings' > 'Add Data' > add access.log to the index = buckelifecycle

  ![image](https://github.com/user-attachments/assets/dafd3d5c-7695-4b3e-967f-cf34b1ef1327)

* Back in 'Indexes', it shows that 3MB of data has been added to the 'bucketlifecycle' index. However, the data had a size of 4 MB, how can this be?

  ![image](https://github.com/user-attachments/assets/5f71465c-3e2c-4459-8dfb-955aa49d9121)

*ANSWER: Because Splunk compressed the data so instead of 4 MB it's 3 MB

  <img width="439" alt="image" src="https://github.com/user-attachments/assets/1cd33b89-6e33-4e55-b4fd-c55735bcfb05" />

### When will data move from Hot Bucket to Warm Bucket?
* We get too many hot buckets
* Hot bucket has not received data since a while
* Timespan of buckets is too larget
* bucket meta-data files have grown large
* index clustering replicaiton error
* splunk is restarted
