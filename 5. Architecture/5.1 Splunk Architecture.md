# Splunk Architecture

## 1. Data Sources

# Parsing Stage

## 2. Parsing Queue

## 3. Parsing Pipeline
**Description:** Where data is converted into events so all the normalization, regex transformation, and others mentioned bloew is done on the parsing pipeline. Extracting the set of default fields for each event, including host, source, and sourcetype.
* source, event typing
* charactr set normalization
* line termination identificaiton
* timestamp identificaiton / normalization, event boundary identification
* regex transfomrs, partse-time field creation
* mask sensitive details in data like credit card numbers.

# Indexing Stage

## Indexes Overview
**Description:** index is a repository of Splunk data.
* Splunk transforms incoming data into events, which it sotres in the indexes.


### 4. Index Queue

### 5. Indexing Pipeline Indexes
**Description:**
* segmentation: breaking events into segments which can be searched upon.
* index building: building index data structures.
* When Splunk indexes data, it creates number of files that fall into two main categories:
  1. The raw data in compressed form (rawdata)
  2. indexes that points to raw data (tsidx files), plus some meta-data files
* writing raw data and index files to disks. 

#### Createomg Index in Splunk ES
* Search 'Settings' > 'Indexes' > 'New Index'
  
  ![image](https://github.com/user-attachments/assets/d3c59c9d-c49f-4d43-992e-8c8b9b5c433a)

* index name = kplabs > press 'save'
  
  ![image](https://github.com/user-attachments/assets/e61f91c5-a759-4c99-8b84-d4d7dafd3ca8)

* index 'kplabs' created

  ![image](https://github.com/user-attachments/assets/ecbc9672-6778-4177-88ff-67bf4ab6650f)

* In Command Line type, **/opt/splunk/var/lib/splunk** and that is the directory where indexes are stores (including kplabs just created).

  ![image](https://github.com/user-attachments/assets/087a72cd-377a-4f08-bc94-50dceb9f9d31)

* Create a sample data in Notepad

  ![image](https://github.com/user-attachments/assets/70af545c-64d4-4458-89c3-ae05eab20be0)

#### Adding Data
* Select 'Settings' > 'Add Data' > Upload

  ![image](https://github.com/user-attachments/assets/372db915-7468-44db-9120-559d95fbb262)

* Select file just created in Notepad > click 'Next'

  ![image](https://github.com/user-attachments/assets/76a9e8cd-d8e2-4c1b-81e4-c67ace5998e7)

* Event break as 'every line' > Next

  ![image](https://github.com/user-attachments/assets/c095df29-ae5a-48b0-9c5b-00d0328c1d20)

* click 'Save'

  ![image](https://github.com/user-attachments/assets/f9fd5c8f-59c1-4a3d-a5c3-1f78e5b1dd86)

* add this data to 'kplabs' index > submit

  ![image](https://github.com/user-attachments/assets/f74c2f39-c92f-43b0-af7d-5c20c3334f19)

* Within 'Search & Reporting' app, events appear

  ![image](https://github.com/user-attachments/assets/df6877e9-f003-4c43-8327-87dc0f3d38be)

* In the Command Line, type **/opt/splunk/var/lib/splunk/kplabs/db/hot_v1_0 ls -l** > you will see **rawdata** directory. Type **cat 0** to view raw data that you created

  <img width="932" alt="image" src="https://github.com/user-attachments/assets/d3054862-0478-413b-9de4-f0affd72f056" />

  <img width="932" alt="image" src="https://github.com/user-attachments/assets/dd4ce8fa-33f2-4e8d-b86f-455ab20c35d3" />

## Splunk Bucket Lifecycle
**Definition:** Splunk stores all its data in directories on server called buckets. Read + Write.
* A bucket moves through several states as it ages
  1. Hot: Most expensive and fast disk. All of the new data is written here and most recent data is kept here. Read Only. CANNOT backup 'Hot' bucket.
  2. Warm: Less expensive disk. Data rolled form hot. Data is not actively writte to warm buckets. Read Only.
  3. Cold: Less expensive disk/storage. Data rolled from warm. Rarely searched data as it has aged / archived. Read Only. Ideally historical data should be stored in Cold Bucket.
  4. Frozen: Data rolled from cold. The data is deleted, but can be archived. Data is deleted by default, but you can configure it to archive it instead. Read Only.
  5. Thawed: If data in frozen bucket is archived, it can be indexed again by thawing. Read Only.

* Hot and Warm bucket are in the same disk.
* Most organizations back-up their data in AWS S3

### Splunk ES Practice

* Create an index called "bucketlifecycle"
  
  ![image](https://github.com/user-attachments/assets/b7a598d2-7754-4dfc-97ae-a77e230f40a4)
  ![image](https://github.com/user-attachments/assets/4b29d468-4a22-4d24-b288-13a5d9ed1bfa)

* Within Command Line, type **/opt/splunk/var/lib/splunk ls -l** and find the index recently created

  ![image](https://github.com/user-attachments/assets/8c247980-e8db-4f13-8595-65c85224d096)

* Within 'bucketlifecycle' index, you'll find colddb (Cold Bucket) and db (Hot/Warm buckets)

  <img width="510" alt="image" src="https://github.com/user-attachments/assets/34545f39-5aa8-4112-8364-5efa252d6eeb" />

* Add Data by clicking on 'Settings' > 'Add Data' > add access.log to the index = buckelifecycle

  ![image](https://github.com/user-attachments/assets/dafd3d5c-7695-4b3e-967f-cf34b1ef1327)

* Back in 'Indexes', it shows that 3MB of data has been added to the 'bucketlifecycle' index. However, the data had a size of 4 MB, how can this be?

  ![image](https://github.com/user-attachments/assets/5f71465c-3e2c-4459-8dfb-955aa49d9121)

*ANSWER: Because Splunk compressed the data so instead of 4 MB it's 3 MB

  <img width="439" alt="image" src="https://github.com/user-attachments/assets/1cd33b89-6e33-4e55-b4fd-c55735bcfb05" />

### When will data move from Hot Bucket to Warm Bucket?
* We get too many hot buckets
* Hot bucket has not received data since a while
* Timespan of buckets is too larget
* bucket meta-data files have grown large
* index clustering replicaiton error
* splunk is restarted

### When will data move from Warm Bucket to Cold Bucket?
* When there are too many warm buckets << you put the limit of how many warm buckets you can have
* Command Line:
   [index_name]
   coldPath = $SPLUNK_DB/$_index_name/colddb
  maxWarmDBCount = 3000
 
### When will data move from Cold Bucket to Frozen Bucket?
**Note:** Data in frozen is no longer searchable as all the data was deleted (unless you configure it not to)
**Note:** Within Frozen Bucket, you only see raw data and not the tsidx file that you would find in Hot and Warm buckets. tsidx file has been removed.
* total size of index (hot+warm+cold) grows too large
* oldest event in bucket exceeds specific age

### Thawing Process
**Note:** This is generally manual process for restoring archived data
* remember you will need to increase the index size if you're looking to restore old data

## Overview of Workflow Actions
* **Description:** automating a flow.
  
* Settings > Fields 
  
  ![image](https://github.com/user-attachments/assets/7efe9c2a-4eb4-44a5-9a01-8ddc1cf30f63)

* Select 'Workflow Actions' > New Workflow Actoins

  ![image](https://github.com/user-attachments/assets/0f77a04b-f453-42f8-9a80-1f41bc715a87)

* fill out information

  ![image](https://github.com/user-attachments/assets/b1668926-49db-49de-bf51-2c22fe8ee36d)

* return to query, open an event > click on 'Event Actions' > whois_lookup

  <img width="949" alt="image" src="https://github.com/user-attachments/assets/58d740c2-24da-4657-a621-eb4aae3fc7d9" />

* It will re-direct user to AbuseIPDB page with it's respective ip address

  ![image](https://github.com/user-attachments/assets/8c6ca601-7f21-48ef-8390-b8afbb3c2a4d)
